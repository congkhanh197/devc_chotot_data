# -*- coding: utf-8 -*-
"""Recommendation System KNN  Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tO3nk7i-NEgTDi-ofzxwl6sPAJvC_d8A
"""



"""#Import package"""

import pandas as pd
import numpy as np

"""#Load dataset"""

from google.colab import drive
drive.mount('/content/drive')

user_item = pd.read_csv('/content/drive/My Drive/Deadline/last_hope.csv')

user_item['area'].value_counts()

temp_matrix = user_item[['price','area','ward','rooms','list_id']]

"""##Preprocessing data"""

temp_matrix['price'] = temp_matrix['price'] / 10000000 # divided 10.000.000

temp_matrix['area'] = temp_matrix['area']*1000 # Tuning

temp_matrix['ward'] = temp_matrix['ward']

temp_matrix['rooms'] = temp_matrix['rooms']

temp_matrix.describe()

filter_adstats = pd.read_csv('/content/drive/My Drive/Deadline/filter_adstats.csv')

count_list_id = dict(filter_adstats['adlist_id'].value_counts())

count_list_id

count_list_id.keys()

mean = int(np.mean(np.array(list(count_list_id.values()))))

median = int(np.median(np.array(list(count_list_id.values()))))

def counter(x):
  return count_list_id.get(x, median)

counter(0)

temp_matrix['count'] = temp_matrix['list_id'].apply(counter)

temp_matrix = temp_matrix.drop(columns = ['list_id'])

temp_matrix

temp_matrix.describe()



from sklearn.preprocessing import Normalizer

normalizer = Normalizer()

temp_matrix

processing_data = normalizer.fit_transform(temp_matrix)

processing_data

normalizer.fit_transform(temp_matrix[:10])

processing_data = normalizer.fit_transform(temp_matrix)

processing_data = np.round(processing_data, 10)

processing_data

def get_col(arr, col):
  return np.array([x[col] for x in arr])

get_col(processing_data, 0)

"""#Model"""

from sklearn.neighbors import NearestNeighbors

nbrs = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1).fit(processing_data)

type(processing_data)

distance, indices = nbrs.kneighbors(processing_data)

len(indices)

len(indices[0])

len(distance)

indices

processing_data[585]

user_item.iloc[0]

for index in range(20):
  print(user_item[['price','price_string','area','ward','rooms']].iloc[indices[0][index]])



from sklearn.externals import joblib

cd /content/drive/My Drive/Deadline

# Save the model as a pickle in a file 
joblib.dump(nbrs, 'recommendation.pkl')

